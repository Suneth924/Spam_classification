{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sunethjayawardana/spam-classification-pyspark?scriptVersionId=160390797\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"20b4dfb8","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-25T13:43:43.10989Z","iopub.status.busy":"2024-01-25T13:43:43.108905Z","iopub.status.idle":"2024-01-25T13:43:44.079759Z","shell.execute_reply":"2024-01-25T13:43:44.078243Z"},"papermill":{"duration":0.982175,"end_time":"2024-01-25T13:43:44.08271","exception":false,"start_time":"2024-01-25T13:43:43.100535","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"e53e4fc5","metadata":{"papermill":{"duration":0.005727,"end_time":"2024-01-25T13:43:44.096704","exception":false,"start_time":"2024-01-25T13:43:44.090977","status":"completed"},"tags":[]},"source":["# This notebook is about building a spam classification using PySpark. Depending on the text the model can predict it as spam or not. The model uses NLP tool kit for predictions."]},{"cell_type":"code","execution_count":2,"id":"f9f357fe","metadata":{"execution":{"iopub.execute_input":"2024-01-25T13:43:44.110725Z","iopub.status.busy":"2024-01-25T13:43:44.110134Z","iopub.status.idle":"2024-01-25T13:44:39.800839Z","shell.execute_reply":"2024-01-25T13:44:39.799163Z"},"papermill":{"duration":55.701215,"end_time":"2024-01-25T13:44:39.80394","exception":false,"start_time":"2024-01-25T13:43:44.102725","status":"completed"},"tags":[]},"outputs":[],"source":["#Installing PySpark \n","!pip install pyspark --quiet"]},{"cell_type":"markdown","id":"47ad65c0","metadata":{"papermill":{"duration":0.005585,"end_time":"2024-01-25T13:44:39.815625","exception":false,"start_time":"2024-01-25T13:44:39.81004","status":"completed"},"tags":[]},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":3,"id":"c213884d","metadata":{"execution":{"iopub.execute_input":"2024-01-25T13:44:39.830026Z","iopub.status.busy":"2024-01-25T13:44:39.829535Z","iopub.status.idle":"2024-01-25T13:44:40.408432Z","shell.execute_reply":"2024-01-25T13:44:40.407206Z"},"papermill":{"duration":0.589763,"end_time":"2024-01-25T13:44:40.411292","exception":false,"start_time":"2024-01-25T13:44:39.821529","status":"completed"},"tags":[]},"outputs":[],"source":["#Apache Spark Libraries\n","import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, split, when\n","from pyspark.ml.feature import VectorAssembler\n","# Import the Decision Tree Classifier class\n","from pyspark.ml.classification  import DecisionTreeClassifier\n","# Import the logistic regression class\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n","from pyspark.sql.functions import regexp_replace\n","from pyspark.ml import Pipeline"]},{"cell_type":"markdown","id":"d74d618a","metadata":{"papermill":{"duration":0.005586,"end_time":"2024-01-25T13:44:40.422845","exception":false,"start_time":"2024-01-25T13:44:40.417259","status":"completed"},"tags":[]},"source":["# Build Spark Session"]},{"cell_type":"code","execution_count":4,"id":"f586e0b0","metadata":{"execution":{"iopub.execute_input":"2024-01-25T13:44:40.436079Z","iopub.status.busy":"2024-01-25T13:44:40.435668Z","iopub.status.idle":"2024-01-25T13:44:46.756047Z","shell.execute_reply":"2024-01-25T13:44:46.752425Z"},"papermill":{"duration":6.331141,"end_time":"2024-01-25T13:44:46.759728","exception":false,"start_time":"2024-01-25T13:44:40.428587","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/01/25 13:44:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]},{"data":{"text/plain":["'3.5.0'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["#Building Spark Session\n","spark = (SparkSession.builder\n","                  .appName('flight')\n","                  .config(\"spark.executor.memory\", \"1G\")\n","                  .config(\"spark.executor.cores\",\"4\")\n","                  .getOrCreate())\n","\n","spark.sparkContext.setLogLevel('WARN')\n","spark.version"]},{"cell_type":"markdown","id":"8060b6fb","metadata":{"papermill":{"duration":0.006068,"end_time":"2024-01-25T13:44:46.772685","exception":false,"start_time":"2024-01-25T13:44:46.766617","status":"completed"},"tags":[]},"source":["# Load Data"]},{"cell_type":"code","execution_count":5,"id":"eb3a8b76","metadata":{"execution":{"iopub.execute_input":"2024-01-25T13:44:46.78809Z","iopub.status.busy":"2024-01-25T13:44:46.787235Z","iopub.status.idle":"2024-01-25T13:44:56.565959Z","shell.execute_reply":"2024-01-25T13:44:56.564738Z"},"papermill":{"duration":9.790665,"end_time":"2024-01-25T13:44:56.569581","exception":false,"start_time":"2024-01-25T13:44:46.778916","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["The data contain 3000 records.\n","+--------------------+-----+\n","|               email|label|\n","+--------------------+-----+\n","| date wed NUMBER ...|    0|\n","|martin a posted t...|    0|\n","|man threatens exp...|    0|\n","|klez the virus th...|    0|\n","| in adding cream ...|    0|\n","+--------------------+-----+\n","only showing top 5 rows\n","\n","[('email', 'string'), ('label', 'int')]\n"]},{"data":{"text/plain":["3000"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Read data from CSV file\n","spam = spark.read.csv('/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv',\n","                         sep=',',\n","                         header=True,\n","                         inferSchema=True,\n","                         nullValue='NA')\n","\n","# Get number of records\n","print(\"The data contain %d records.\" % spam.count())\n","\n","# View the first five records\n","spam.show(5)\n","\n","# Check column data types\n","print(spam.dtypes)\n","spam.count()"]},{"cell_type":"code","execution_count":6,"id":"5ffbb41f","metadata":{"execution":{"iopub.execute_input":"2024-01-25T13:44:56.590701Z","iopub.status.busy":"2024-01-25T13:44:56.5901Z","iopub.status.idle":"2024-01-25T13:44:56.704196Z","shell.execute_reply":"2024-01-25T13:44:56.702689Z"},"papermill":{"duration":0.128741,"end_time":"2024-01-25T13:44:56.707767","exception":false,"start_time":"2024-01-25T13:44:56.579026","status":"completed"},"tags":[]},"outputs":[],"source":["# Drop rows with missing email values\n","spam = spam.dropna(subset=['email'])\n","# Remove punctuation (REGEX provided) and numbers\n","wrangled = spam.withColumn('email', regexp_replace(spam.email, '[_():;,.!?\\\\-]', ' '))\n","wrangled = wrangled.withColumn('email', regexp_replace(wrangled.email, '[0-9]', ' '))\n","\n","# Merge multiple spaces\n","wrangled = wrangled.withColumn('email', regexp_replace(wrangled.email, ' +', ' '))"]},{"cell_type":"code","execution_count":7,"id":"c8967574","metadata":{"execution":{"iopub.execute_input":"2024-01-25T13:44:56.728974Z","iopub.status.busy":"2024-01-25T13:44:56.72837Z","iopub.status.idle":"2024-01-25T13:44:57.046972Z","shell.execute_reply":"2024-01-25T13:44:57.045231Z"},"papermill":{"duration":0.333662,"end_time":"2024-01-25T13:44:57.050832","exception":false,"start_time":"2024-01-25T13:44:56.71717","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+-----+\n","|               email|label|\n","+--------------------+-----+\n","| date wed NUMBER ...|    0|\n","|martin a posted t...|    0|\n","|man threatens exp...|    0|\n","|klez the virus th...|    0|\n","| in adding cream ...|    0|\n","| i just had to ju...|    0|\n","|the scotsman NUMB...|    0|\n","|martin adamson wr...|    0|\n","|the scotsman thu ...|    0|\n","|i have been tryin...|    0|\n","|hello have you se...|    0|\n","|yes great minds t...|    0|\n","|on mon aug NUMBER...|    0|\n","| from chris garri...|    0|\n","|spamassassin is h...|    0|\n","|hi all apologies ...|    0|\n","| in forteana y d ...|    0|\n","|in a nutshell sol...|    0|\n","|apols if this has...|    0|\n","|can someone expla...|    0|\n","+--------------------+-----+\n","only showing top 20 rows\n","\n"]}],"source":["wrangled.show()"]},{"cell_type":"markdown","id":"db724e38","metadata":{"papermill":{"duration":0.008905,"end_time":"2024-01-25T13:44:57.06911","exception":false,"start_time":"2024-01-25T13:44:57.060205","status":"completed"},"tags":[]},"source":["# Initiating Pipeline\n","* split the text into tokens\n","* remove stop words\n","* applie the hashing trick\n","* convert the data from counts to IDF\n","* train a logistic regression model."]},{"cell_type":"code","execution_count":8,"id":"53f8fea0","metadata":{"execution":{"iopub.execute_input":"2024-01-25T13:44:57.087474Z","iopub.status.busy":"2024-01-25T13:44:57.086926Z","iopub.status.idle":"2024-01-25T13:44:57.320219Z","shell.execute_reply":"2024-01-25T13:44:57.31895Z"},"papermill":{"duration":0.245611,"end_time":"2024-01-25T13:44:57.323571","exception":false,"start_time":"2024-01-25T13:44:57.07796","status":"completed"},"tags":[]},"outputs":[],"source":["# Break text into tokens at non-word characters\n","tokenizer = Tokenizer(inputCol='email', outputCol='words')\n","\n","# Remove stop words\n","remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='terms')\n","\n","# Apply the hashing trick and transform to TF-IDF\n","hasher = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"hash\")\n","idf = IDF(inputCol=hasher.getOutputCol(), outputCol=\"features\")\n","\n","# Create a logistic regression object and add everything to a pipeline\n","logistic = LogisticRegression(featuresCol=idf.getOutputCol(),labelCol=\"label\")\n","pipeline = Pipeline(stages=[tokenizer, remover, hasher,  idf, logistic])"]},{"cell_type":"code","execution_count":9,"id":"af6e9736","metadata":{"execution":{"iopub.execute_input":"2024-01-25T13:44:57.341002Z","iopub.status.busy":"2024-01-25T13:44:57.339159Z","iopub.status.idle":"2024-01-25T13:45:19.793571Z","shell.execute_reply":"2024-01-25T13:45:19.792258Z"},"papermill":{"duration":22.467024,"end_time":"2024-01-25T13:45:19.797816","exception":false,"start_time":"2024-01-25T13:44:57.330792","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["24/01/25 13:45:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n","24/01/25 13:45:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n","24/01/25 13:45:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"]}],"source":["train_data, test_data = wrangled.randomSplit([0.8, 0.2])\n","model = pipeline.fit(train_data)"]},{"cell_type":"code","execution_count":10,"id":"043bdc30","metadata":{"execution":{"iopub.execute_input":"2024-01-25T13:45:19.826998Z","iopub.status.busy":"2024-01-25T13:45:19.826534Z","iopub.status.idle":"2024-01-25T13:45:20.136119Z","shell.execute_reply":"2024-01-25T13:45:20.134863Z"},"papermill":{"duration":0.326853,"end_time":"2024-01-25T13:45:20.140863","exception":false,"start_time":"2024-01-25T13:45:19.81401","status":"completed"},"tags":[]},"outputs":[],"source":["predictions = model.transform(test_data)"]},{"cell_type":"code","execution_count":11,"id":"a2a9f397","metadata":{"execution":{"iopub.execute_input":"2024-01-25T13:45:20.166788Z","iopub.status.busy":"2024-01-25T13:45:20.16619Z","iopub.status.idle":"2024-01-25T13:45:22.693233Z","shell.execute_reply":"2024-01-25T13:45:22.692018Z"},"papermill":{"duration":2.544151,"end_time":"2024-01-25T13:45:22.697831","exception":false,"start_time":"2024-01-25T13:45:20.15368","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["24/01/25 13:45:20 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Area Under ROC: 0.9868070685064162\n"]}],"source":["evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n","area_under_curve = evaluator.evaluate(predictions)\n","print(\"Area Under ROC:\", area_under_curve)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":91827,"sourceId":213216,"sourceType":"datasetVersion"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":106.752438,"end_time":"2024-01-25T13:45:25.343013","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-25T13:43:38.590575","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}